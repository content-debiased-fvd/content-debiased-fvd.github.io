<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content=".">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>On the Content Bias in Fréchet Video Distance</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">On the Content Bias in Fréchet Video Distance
          </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://songweige.github.io/">Songwei
                Ge</a><sup>1</sup>&nbsp</span>
            <span class="author-block">
              <a href="https://anime26398.github.io/">Aniruddha Mahapatra</a><sup>2, 3</sup>&nbsp
            </span>
            <span class="author-block">
              <a href="https://gauravparmar.com/">Gaurav Parmar</a><sup>2</sup>&nbsp
            </span>
            <span class="author-block">
              <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan
                Zhu</a><sup>2</sup>&nbsp</span>
            <span class="author-block">
              <a href="https://jbhuang0604.github.io/">Jia-Bin
                Huang</a><sup>1</sup>&nbsp
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Maryland, College Park
              &nbsp</span>
            <span class="author-block"><sup>2</sup>Carnegie
              Mellon University</span>
            <span class="author-block"><sup>3</sup>Adobe Research &nbsp</span>
          </div>


          <div class="is-size-5 publication-venue">
            CVPR 2024
          </div>

          </div>
        </div>
      </div>
      <div class="column has-text-centered">
        <div class="publication-links">
          <!-- PDF Link. -->
          <span class="link-block">
            <a href="https://arxiv.org/abs/" class="external-link button is-normal is-rounded">
              <span class="icon">
                <i class="ai ai-arxiv"></i>
              </span>
              <span>arXiv</span>
            </a>
          </span>
          <span class="link-block">
            <a href="https://github.com/songweige/content-debiased-fvd"
              class="external-link button is-normal is-rounded">
              <span class="icon">
                <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false"
                  data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg"
                  viewBox="0 0 496 512" data-fa-i2svg="">
                  <path fill="currentColor"
                    d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z">
                  </path>
                </svg>
              </span>
              <span>Code</span>
            </a>
          </span>
          <span class="link-block">
            <a href="./#bibtex"
              class="external-link button is-normal is-rounded">
              <span class="icon">
                <i class="ai ai-obp"></i>
              </span>
              <span>BibTex</span>
            </a>
          </span>
      </div>
    </div>
  </div>
</section>

<section class="hero" style="margin-bottom: 50px;margin-top: 0px;">
  <div class="container is-max-desktop">
      <div class="columns is-centered">
      <div class="column">
        <div class="has-text-justified">
          <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
            <source src="./static/videos/sky_clean.mp4" type="video/mp4">
          </video>
          <p class="has-text-centered">
              Reference Videos
          </p>
        </div>
      </div>
      <div class="column">
        <div class="content has-text-centered">
          <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
            <source src="./static/videos/sky_Motion Blur Spatial_5.mp4" type="video/mp4">
          </video>
          <p class="has-text-centered">
            Medium Spatial Corruption <br> No Temporal Corruption <br>
            FVD=317.10
          </p>
        </div>
      </div>
      <div class="column">
        <div class="content has-text-centered">
          <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
            <source src="./static/videos/sky_Motion Blur_3.mp4" type="video/mp4">
          </video>
          <p class="has-text-centered">
            Small Spatial Corruption <br> Severe Temporal Corruption <br>
            FVD=310.52
          </p>
        </div>
      </div>
    </div>
    <p class="blog-text">
      <b class="topic"> FVD is biased towards per-frame quality than temporal consistency. </b> FVD is the primary metric for evaluating 
      video generation models. Ideally, such a metric should capture <i>both</i> spatial and temporal aspects.
      However, our experiments reveal a strong bias toward individual frame quality.
      
      <br><br> As in this simple test, we compare a reference set of videos 
      (left) to two different sets of corruptions. The first corruption set introduces mild spatial corruption 
      (middle), which results in an FVD score of 317.10. In contrast, the second set induces slightly less spatial corruption and 
      additional temporal inconsistency (right), 
      yet results in a <i>lower (better)</i> FVD score of 310.52. This discrepancy highlights the metric's bias towards spatial quality.
      In the following, we show experiments to quantify such <i>content bias</i> and understand the origin, progressively adapting from synthetic to real-world settings.
    </p>
  </div>
  </section>

  <hr>
  <section class="hero" style="margin-bottom: 50px;margin-top: 50px;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div style="width: 16%;">
        </div>
        <div style="width: 33%;">
          <p class="has-text-centered">
            Corruptions
          </p>
        </div>
        <div style="width: 16%;">
        </div>
        <div style="width: 33%;">
          <p class="has-text-centered">
            Corruptions
          </p>
        </div>
      </div>
        <div class="columns is-centered">
        <div class="column">
          <div class="has-text-justified">
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig3_elastic_clean.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
                Original Video
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content has-text-centered">
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig3_elastic_s.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              Spatial only
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content has-text-centered">
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig3_elastic_st.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              Spatiotemporal
            </p>
          </div>
        </div>
        <div class="column">
          <div class="has-text-justified">
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig3_motion_blur_clean.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
                Original Video
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content has-text-centered">
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig3_motion_blur_s.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              Spatial only
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content has-text-centered">
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig3_motion_blur_st.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              Spatiotemporal
            </p>
          </div>
        </div>
      </div>
      <div class="content has-text-centered">
        <img src="./static/images/fid_fvd.png" width="200%">
      </div>
      <p class="blog-text">
        <b class="topic"> Quantify the temporal sensitivity. </b> 
        We first develop ways to distort videos so that the frame quality deteriorates the same while the temporal quality 
        is either intact or significantly decreased. 
        By comparing the FVD induced by the spatiotemporal corruption against the spatial corruption, we can analyze FVD's reletive 
        sensitivity to the temporal aspect. 
        
        <br><br>We first verify our claim that the two distorted video sets share similar frame quality with the minimal FID difference, 
        shown in the table, between the spatial and spatiotemporal distortion across different datasets.
        We then find that FVD sometimes fails to detect the temporal quality decrease induced by
        spatiotemporal corruption. For example, the temporal inconsistency in the FaceForensics dataset only raises FVD by 3.6%. 
        To further grasp the significance of the FVD increase due to temporal inconsistency, 
        we compare it with the FVD computed using different models below.
      </p>
    </div>
  </section>

  <hr>
  <section class="hero" style="margin-bottom: 50px;margin-top: 50px;">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <img src="./static/images/cause.png" width="100%">
      </div>
      <p class="blog-text">
        <b class="topic"> Origin of FVD's content bias. </b> FVD employs an Inflated 3D ConvNet (I3D) model, 
          initially trained for action recognition task on the Kinetics-400 dataset, which is known to be biased to the 
          static features in the content instead of motions. We thus conjecture that the FVD bias can be attributed to 
          the features extracted from such supervised video classifier. 
          
          <br><br> To unravel the confounding factors including the model architecture, training objectives, model capacity, and the dataset,
          we further delve into a comparative study with different trained models, where the differences of these models are summarized in the table.
          As shown in the Figure on the left, we find that using features from self-supervised models boosts FVD's temporal sensitivity.
          Training on the content-debiased data further helps mitigate the bias.
      </p>
    </div>
  </section>


  <hr>
  <section class="hero" style="margin-bottom: 50px;margin-top: 50px;">
    <div class="container is-max-desktop">
      <div class="content has-text-centered">
        <img src="./static/images/null_space.png" width="100%">
      </div>
      <p class="blog-text">
        <b class="topic">Without improving the temporal quality of the generated videos, the FVD scores can still be decreased. </b>
        We now move from synthetic corruptions to generated videos.
        We follow <a href="https://arxiv.org/abs/1904.06991">Kynkäänniemi et al.</a> 
        to probe the FVD perceptual null space, namely the space where the temporal quality of
        generated videos remains unchanged while the FVD score can be effectively adjusted. To do so, we first generate a larger candidate set of
        videos <i>without any motions</i>. We then meticulously sample from this set to induce a decrease in FVD (denoted as FVD*).
        <br><br>As in the table above, across different video generators and training datasets,
        despite the absence of motion in the generated videos, one can still reduce FVD by up to half through selectively
        choosing from the candidate videos. Conversely, when computing the features 
        for FVD using the VideoMAE-v2 model, which is sensitive to temporal quality, the observed gaps significantly diminish, and the FVD 
        scores can hardly be decreased through resampling.
        <br><br>We now have concluded that FVD is highly insensitive to the temporal quality and consistency of the generated videos, and verified the 
        hypothesis that the bias originates from the content-biased video features and show that self-supervised features can mitigate
        the issues. Next, we extend our study to real-world examples.
      </p>
    </div>
  </section>

  <hr>
  <section class="hero" style="margin-bottom: 50px;margin-top: 50px;">
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="align-items: center;">
        <div class="column">
          <div class="has-text-justified">
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig9_stylegan-v_default.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              Default StyleGAN-v.
            </p>
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig9_stylegan-v_lstm16.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              StyleGAN-v with LSTM motion codes.
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content has-text-centered">
            <img src="./static/images/styleganv.png" width="100%">
          </div>
        </div>
      </div>
      <p class="blog-text">
        <b class="topic">Case study I. </b> The
        default <a href="https://arxiv.org/abs/2112.14683">StyleGAN-v</a> model generates natural motions while its variant
        with LSTM motion codes generates repeated temporal patterns. However, previous study found that the FVD metric fails to capture the variant's worse quality.
        We observe the same trend as shown in the table on the right.
        Upon computing FVD using VideoMAE-v2 features, we have them to be in accordance with human judgment.
      </p>
    </div>
  </section>


  <hr>
  <section class="hero" style="margin-bottom: 50px;margin-top: 50px;">
    <div class="container is-max-desktop">
      <div class="columns is-centered" style="align-items: center;">
        <div class="column">
          <div class="has-text-justified">
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig10_digan_0_16.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              Frames 0 - 16.
            </p>
            <video id="dollyzoom" autoplay="" controls="" muted="" loop="" playsinline="" height="100%">
              <source src="./static/videos/fig10_digan_128_144.mp4" type="video/mp4">
            </video>
            <p class="has-text-centered">
              Frames 128 - 144.
            </p>
          </div>
        </div>
        <div class="column">
          <div class="content has-text-centered">
            <img src="./static/images/digan.png" width="100%">
          </div>
        </div>
      </div>
      <p class="blog-text">
        <b class="topic">Case study II. </b> The initial 16 frames generated by the <a href="https://arxiv.org/abs/2202.10571">DIGAN</a>
          model exhibit natural motions, while the extrapolated frames contain periodic spatiotemporal
          artifacts. Similarly, previous paper noticed that the FVD fails to distinguish between the two as confirmed by the table. Instaed,
          FVD computed using VideoMAE features better follows human judgment.
      </p>
    </div>
  </section>

  <section class="hero" style="margin-bottom: 50px;margin-top: 50px;">
    <div class="container is-max-desktop">
    <p>
      <b class="topic">FVD tookit. </b> We develop code and provide pre-computed features for computing FVD with 
      different feature extractors. The toolkit is available at <a href="https://github.com/songweige/content-debiased-fvd">Github repo</a>.
    </p>
    </div>
  </section>

  <section class="hero" style="margin-bottom: 50px;margin-top: 50px;">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Acknowledgment</h2>
      <div class="content has-text-justified">
        <p>
          We thank Angjoo Kanazawa, Aleksander Holynski, Devi Parikh, and Yogesh Balaji for the early feedback and discussion.
          We thank Or Patashnik, Richard Zhang, and Hadi Alzayer for their helpful comments and paper proofreading.
          We thank Ivan Skorokhodov for his help with reproducing the StyleGAN-v ablation experiments.
          This work is partly supported by NSF grant No. IIS-239076, the Packard Fellowship, as well as NSF grants No. IIS-1910132 and IIS-2213335. 
        </p>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title"><a id="bibtex">BibTeX</a></h2>
      <pre><code>@inproceedings{ge2024content,
      title={On the Content Bias in Fréchet Video Distance},
      author={Ge, Songwei and Mahapatra, Aniruddha and Parmar, Gaurav and Zhu, Jun-Yan and Huang, Jia-Bin},
      booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
      year={2024}
}</code></pre>
    </div>
  </section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Website templated borrowed from this <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
